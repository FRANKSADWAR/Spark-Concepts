{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967431e6-35fd-428a-afd2-9e94ef29d117",
   "metadata": {},
   "source": [
    "# The Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8c970-8754-4096-a30e-d44716c97f86",
   "metadata": {},
   "source": [
    "Spark applications consist of **Driver Process** and **Executor process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b4207-0ca6-456c-9878-2ceb21bcce17",
   "metadata": {},
   "source": [
    "Driver process: \n",
    "- Is responsible for distributing, analysing and scheduling work across the executors\n",
    "- Is responsible for maintaining information about the Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74761dd-bb51-429d-b1c1-a5b2264b9cd0",
   "metadata": {},
   "source": [
    "Executor process:\n",
    "- Carries the actual work that is assigned to them by the driver process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8303a48a-a853-4a9a-a036-b8c307a1e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc36e49-bb75-433b-9f5b-e7840f0911da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/24 00:17:08 WARN Utils: Your hostname, billy-Yoga-9-15IMH5, resolves to a loopback address: 127.0.1.1; using 10.107.122.146 instead (on interface wlp0s20f3)\n",
      "26/02/24 00:17:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/24 00:17:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark =  SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364cb19e-2f8b-4ba2-b111-92d7c62be34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.107.122.146:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7cf78c6c27b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6704c5-b7c5-44d5-86b2-eff76e9c2e84",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d1014-9906-4592-ae51-27f226ac6e92",
   "metadata": {},
   "source": [
    " A PySpark DataFrame can be created via: \n",
    "- createDataFrame method passing a list of lists, tuples, dictionaries, pyspark.sql.Row\n",
    "- From a pandas dataframe\n",
    "- an RDD\n",
    "- Reading a file, database (JDBC Driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144f706f-9bb4-46d1-9736-1b238d821654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb03aa5-8d64-479a-bf6e-590a8f12bc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: timestamp, e: timestamp]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=0.45, c='Homer', d=datetime.today(), e=datetime.now()),\n",
    "    Row(a=1, b=0.89, c='Marcus Aurellius', d=datetime.today(), e=datetime.now()),\n",
    "    Row(a=1, b=0.78, c='Ceasar', d=datetime.today(), e=datetime.now())\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb8eaac-86a3-4ca4-bafa-f02b6db157e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  a|   b|               c|                   d|                   e|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  1|0.45|           Homer|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "|  1|0.89|Marcus Aurellius|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "|  1|0.78|          Ceasar|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ca66d8-da4a-4d12-b112-281fc9b49a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, a: string, b: string, c: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826f9e3f-1751-4f28-b95d-958f9f001c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: timestamp (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17de70b-0cf2-45cb-ab33-0ca5ff62aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   d|\n",
      "+--------------------+\n",
      "|2026-02-19 22:27:...|\n",
      "|2026-02-19 22:27:...|\n",
      "|2026-02-19 22:27:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"d\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed9b20-d517-43c9-b809-18b6cf1a3fd1",
   "metadata": {},
   "source": [
    "##### We can also register the DataFrame as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be436a85-a82f-4bba-aac1-0f3c62f3f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f4240a-4a4a-4466-a34f-c86192348012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  a|   b|               c|                   d|                   e|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  1|0.45|           Homer|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "|  1|0.89|Marcus Aurellius|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "|  1|0.78|          Ceasar|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqldf = spark.sql(\"SELECT * FROM df_table\")\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3527380-f11e-457a-8cb1-13fbd5d2eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  a|   b|               c|                   d|                   e|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "|  1|0.89|Marcus Aurellius|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "|  1|0.78|          Ceasar|2026-02-19 22:27:...|2026-02-19 22:27:...|\n",
      "+---+----+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_table WHERE b > 0.5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fa585-df2f-4a5c-a5ad-65f79742bf4f",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f98e7db6-a2a0-450c-bf26-78dadfcb65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"data/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa5a272c-d535-429f-9b5d-38eb0675e34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/billy/Documents/Spark_concepts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a9c3b4-be62-48cc-ae32-11d196254adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2faf37b3-10cd-4420-804b-e12cfd20633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "flight_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d171f6-b21a-40b6-95ad-99dbba4789af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad68df-234d-49cb-b60c-9c6d5c51e5c6",
   "metadata": {},
   "source": [
    "#### Reading, sorting and collecting a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2448585a-3306-486f-be56-a2a1c387d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#147 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#147 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=100]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#145,ORIGIN_COUNTRY_NAME#146,count#147] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/billy/Documents/Spark_concepts/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da530f5-5fed-4764-8429-40155292cab4",
   "metadata": {},
   "source": [
    "- Nothing happens to the data when we call sort because its just a transformation.\n",
    "- Sort does not modify the DataFrame, sort is a transformation that returns a new DataFrame by transforming the previous DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d11a7d-838c-4bb1-ae00-489b0b21d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb8502a8-28e9-437d-9892-28a5b3cad5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data.sort(\"count\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75852e1-29b4-425c-8b4d-b8d0894e7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data.createOrReplaceTempView(\"flight_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2048d872-5dce-47b7-bd15-6d7bbcc1c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, \n",
    "                    count(1) FROM flight_data_table \n",
    "                    GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
    "\n",
    "dfWay = flight_data.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd2d264-6e79-40d7-a206-3efa053e7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#145, 10), ENSURE_REQUIREMENTS, [plan_id=128]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#145] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/billy/Documents/Spark_concepts/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#145, 10), ENSURE_REQUIREMENTS, [plan_id=141]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#145] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/billy/Documents/Spark_concepts/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()\n",
    "dfWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dfb7381-cf2e-42f5-ac1f-172f633b1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT max(count) from flight_data_table\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c63a6f9e-c652-4bfe-a0fe-e669961cfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cbb0b21-5c53-45cb-8732-e10876064233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12d7e74d-59cd-47da-92a6-ad98400c06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSql = spark.sql(\"\"\"SELECT \n",
    "            DEST_COUNTRY_NAME, \n",
    "            SUM(count) AS destination_total FROM flight_data_table  \n",
    "            GROUP BY DEST_COUNTRY_NAME \n",
    "            ORDER BY destination_total DESC LIMIT 5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06447777-0ad5-461b-87ca-38a3c48e41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#202L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#145,destination_total#202L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[sum(count#147)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#145, 10), ENSURE_REQUIREMENTS, [plan_id=223]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#145], functions=[partial_sum(count#147)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#145,count#147] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/billy/Documents/Spark_concepts/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1765e65c-d9d2-4c65-a52b-854e74e92790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f249a4-69ed-4ed4-a3a4-fd748ee00e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58401d7b-e9a3-4a45-9b82-02670280562e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ea44d-ea1d-4dcb-9995-72f2c082c3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e70749-5f24-4d11-9896-6da7612c4411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b9296-3014-4ef7-a1fe-b8f4c0cbdbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465efe9a-16f0-4eae-8dbd-678c3ff8d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d19065-b4e0-4616-896f-d5fbe22bc95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc367ad-35f9-40b4-8346-ab4bb849a28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
